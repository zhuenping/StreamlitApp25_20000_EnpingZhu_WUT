"""
Complete Data I/O Utilities | å®Œæ•´æ•°æ®è¾“å…¥è¾“å‡ºå·¥å…·
- Raw Data Loading | åŸå§‹æ•°æ®åŠ è½½
- File Encoding Detection | æ–‡ä»¶ç¼–ç æ£€æµ‹
- Data Caveats & Documentation | æ•°æ®è¯´æ˜æ–‡æ¡£
"""
import pandas as pd
import os
import chardet
from typing import Dict, Optional  # æ–°å¢ï¼šå¯¼å…¥Dictç±»å‹

# æ ¸å¿ƒå­—æ®µå®šä¹‰ï¼ˆç¡®ä¿åŸå§‹æ•°æ®åŒ…å«è¿™äº›å­—æ®µï¼‰
REQUIRED_FIELDS = [
    "Date_of_Onset",    # å‘ç—…æ—¥æœŸ
    "Location",         # åŒºåŸŸ
    "SES",              # ç¤¾ä¼šç»æµæ°´å¹³
    "Chronic_Conditions",# æ…¢æ€§ç—…çŠ¶æ€
    "Vaccination_Status",# æ¥ç§çŠ¶æ€
    "Daily_New_Cases",  # æ¯æ—¥æ–°å¢ç—…ä¾‹
    "Hospital_Capacity",# åŒ»é™¢å®¹é‡
    "Hospitalization_Requirement",# ä½é™¢éœ€æ±‚
    "Immunity_Level",   # å…ç–«æ°´å¹³
    "Age"               # å¹´é¾„
]

def detect_file_encoding(file_path: str) -> str:
    """æ£€æµ‹æ–‡ä»¶ç¼–ç ï¼ˆè§£å†³ä¸­æ–‡/ç‰¹æ®Šå­—ç¬¦ä¹±ç é—®é¢˜ï¼‰"""
    try:
        with open(file_path, "rb") as file:
            # è¯»å–å‰10000å­—èŠ‚ç”¨äºç¼–ç æ£€æµ‹
            raw_data = file.read(10000)
            # æ£€æµ‹ç¼–ç 
            encoding_result = chardet.detect(raw_data)
        # è¿”å›ç¼–ç ï¼ˆé»˜è®¤utf-8ï¼‰
        detected_encoding = encoding_result["encoding"] or "utf-8"
        print(f"ğŸ” Detected file encoding: {detected_encoding} | æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç ï¼š{detected_encoding}")
        return detected_encoding
    except Exception as e:
        raise RuntimeError(f"Failed to detect file encoding: {str(e)} | æ£€æµ‹æ–‡ä»¶ç¼–ç å¤±è´¥ï¼š{str(e)}") from e

def load_raw_dataset(data_dir: str = "data", file_name: str = "public_health_surveillance_dataset.csv") -> pd.DataFrame:
    """åŠ è½½åŸå§‹æ•°æ®é›†ï¼ˆæ ¡éªŒè·¯å¾„ã€å­—æ®µã€æ•°æ®è´¨é‡ï¼‰"""
    # 1. æ„å»ºæ•°æ®è·¯å¾„
    project_root = os.getcwd()
    data_file_path = os.path.join(project_root, data_dir, file_name)
    
    # 2. æ ¡éªŒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file_path):
        raise FileNotFoundError(
            f"âŒ Raw dataset not found! Expected path: {data_file_path} | åŸå§‹æ•°æ®é›†æœªæ‰¾åˆ°ï¼é¢„æœŸè·¯å¾„ï¼š{data_file_path}\n"
            f"Please place the CSV file in the '{data_dir}' folder at the project root. | è¯·å°†CSVæ–‡ä»¶æ”¾å…¥é¡¹ç›®æ ¹ç›®å½•çš„'{data_dir}'æ–‡ä»¶å¤¹ã€‚"
        )
    
    # 3. æ£€æµ‹æ–‡ä»¶ç¼–ç å¹¶è¯»å–æ•°æ®
    try:
        file_encoding = detect_file_encoding(data_file_path)
        # è¯»å–CSVæ–‡ä»¶ï¼ˆè§£ææ—¥æœŸå­—æ®µï¼‰
        raw_df = pd.read_csv(
            data_file_path,
            encoding=file_encoding,
            parse_dates=["Date_of_Onset"],  # è‡ªåŠ¨è§£ææ—¥æœŸ
            low_memory=False  # å¤„ç†å¤§æ–‡ä»¶æ—¶é¿å…ç±»å‹æ¨æ–­é”™è¯¯
        )
    except Exception as e:
        raise RuntimeError(
            f"âŒ Failed to load raw dataset: {str(e)} | åŠ è½½åŸå§‹æ•°æ®é›†å¤±è´¥ï¼š{str(e)}\n"
            f"Suggestions | å»ºè®®ï¼š\n"
            f"1. Check if the CSV format is valid (no extra commas/newlines). | æ£€æŸ¥CSVæ ¼å¼æ˜¯å¦æœ‰æ•ˆï¼ˆæ— å¤šä½™é€—å·/æ¢è¡Œï¼‰ã€‚\n"
            f"2. Verify the file encoding (try 'utf-8' or 'gbk'). | éªŒè¯æ–‡ä»¶ç¼–ç ï¼ˆå°è¯•'utf-8'æˆ–'gbk'ï¼‰ã€‚"
        ) from e
    
    # 4. æ ¡éªŒæ ¸å¿ƒå­—æ®µæ˜¯å¦é½å…¨
    missing_fields = [field for field in REQUIRED_FIELDS if field not in raw_df.columns]
    if missing_fields:
        raise ValueError(
            f"âŒ Missing required fields in raw dataset: {missing_fields} | åŸå§‹æ•°æ®é›†ç¼ºå¤±æ ¸å¿ƒå­—æ®µï¼š{missing_fields}\n"
            f"Required fields must include: {REQUIRED_FIELDS} | æ ¸å¿ƒå­—æ®µå¿…é¡»åŒ…å«ï¼š{REQUIRED_FIELDS}"
        )
    
    # 5. æ ¡éªŒæ•°æ®é‡ï¼ˆé¿å…è¿‡å°æ•°æ®é›†ï¼‰
    min_data_rows = 500
    if len(raw_df) < min_data_rows:
        raise ValueError(
            f"âŒ Raw dataset is too small (only {len(raw_df)} rows). Minimum required: {min_data_rows} rows. | åŸå§‹æ•°æ®é›†è¿‡å°ï¼ˆä»…{len(raw_df)}è¡Œï¼‰ã€‚æœ€ä½è¦æ±‚ï¼š{min_data_rows}è¡Œ\n"
            f"Please use a dataset with sufficient sample size for reliable analysis. | è¯·ä½¿ç”¨æ ·æœ¬é‡è¶³å¤Ÿçš„æ•°æ®é›†ä»¥ç¡®ä¿åˆ†æå¯é æ€§ã€‚"
        )
    
    # 6. æ‰“å°åŠ è½½æˆåŠŸä¿¡æ¯
    print(f"âœ… Raw dataset loaded successfully! | åŸå§‹æ•°æ®é›†åŠ è½½æˆåŠŸï¼")
    print(f"Dataset Size: {len(raw_df)} rows Ã— {len(raw_df.columns)} columns | æ•°æ®é›†å¤§å°ï¼š{len(raw_df)}è¡Œ Ã— {len(raw_df.columns)}åˆ—")
    print(f"Date Range: {raw_df['Date_of_Onset'].min().date()} ~ {raw_df['Date_of_Onset'].max().date()} | æ—¥æœŸèŒƒå›´ï¼š{raw_df['Date_of_Onset'].min().date()} ~ {raw_df['Date_of_Onset'].max().date()}")
    
    return raw_df

def get_data_caveats() -> str:
    """è·å–æ•°æ®é›†è¯´æ˜æ–‡æ¡£ï¼ˆä¸­è‹±æ–‡åŒæ³¨é‡Šï¼‰"""
    return """
    ### Dataset Caveats & Documentation | æ•°æ®é›†è¯´æ˜ä¸æ–‡æ¡£
    #### 1. Data Source | æ•°æ®æ¥æº
    - Public health surveillance records from 2023 to 2024, covering Urban, Suburban, and Rural regions.
      2023-2024å¹´å…¬å…±å«ç”Ÿç›‘æµ‹è®°å½•ï¼Œè¦†ç›–åŸå¸‚ã€éƒŠåŒºã€å†œæ‘ä¸‰ç±»åŒºåŸŸã€‚
    - Collected from multiple regional health departments (de-identified to protect privacy).
      æ•°æ®æ¥è‡ªå¤šä¸ªåŒºåŸŸå«ç”Ÿéƒ¨é—¨ï¼ˆå·²å»æ ‡è¯†åŒ–ï¼Œä¿æŠ¤éšç§ï¼‰ã€‚

    #### 2. Key Field Definitions | æ ¸å¿ƒå­—æ®µå®šä¹‰
    | Field Name | å­—æ®µåç§° | Definition | å®šä¹‰ |
    |------------|----------|------------|------|
    | Date_of_Onset | å‘ç—…æ—¥æœŸ | Date when the patient was diagnosed (core time dimension). | æ‚£è€…ç¡®è¯Šæ—¥æœŸï¼ˆæ ¸å¿ƒæ—¶é—´ç»´åº¦ï¼‰ã€‚ |
    | Location | åŒºåŸŸ | Region type (Urban/Suburban/Rural, case-insensitive). | åŒºåŸŸç±»å‹ï¼ˆåŸå¸‚/éƒŠåŒº/å†œæ‘ï¼Œä¸åŒºåˆ†å¤§å°å†™ï¼‰ã€‚ |
    | SES | ç¤¾ä¼šç»æµæ°´å¹³ | Socio-Economic Status (High/Medium/Low, based on household income). | ç¤¾ä¼šç»æµæ°´å¹³ï¼ˆé«˜/ä¸­/ä½ï¼ŒåŸºäºå®¶åº­æ”¶å…¥ï¼‰ã€‚ |
    | Chronic_Conditions | æ…¢æ€§ç—…çŠ¶æ€ | Whether the patient has chronic diseases (1=Yes, 0=No). | æ‚£è€…æ˜¯å¦æœ‰æ…¢æ€§ç—…ï¼ˆ1=æ˜¯ï¼Œ0=å¦ï¼‰ã€‚ |
    | Vaccination_Status | æ¥ç§çŠ¶æ€ | Whether the patient is vaccinated (1=Vaccinated, 0=Unvaccinated). | æ‚£è€…æ˜¯å¦æ¥ç§ç–«è‹—ï¼ˆ1=å·²æ¥ç§ï¼Œ0=æœªæ¥ç§ï¼‰ã€‚ |
    | Daily_New_Cases | æ¯æ—¥æ–°å¢ç—…ä¾‹ | Number of new confirmed cases per day (non-negative). | æ¯æ—¥æ–°å¢ç¡®è¯Šç—…ä¾‹æ•°ï¼ˆéè´Ÿï¼‰ã€‚ |
    | Hospital_Capacity | åŒ»é™¢å®¹é‡ | Total hospital beds in the region (â‰¥10 beds). | åŒºåŸŸå†…åŒ»é™¢æ€»åºŠä½æ•°ï¼ˆâ‰¥10å¼ ï¼‰ã€‚ |
    | Hospitalization_Requirement | ä½é™¢éœ€æ±‚ | Number of patients requiring hospitalization per day (non-negative). | æ¯æ—¥éœ€è¦ä½é™¢çš„æ‚£è€…æ•°ï¼ˆéè´Ÿï¼‰ã€‚ |
    | Immunity_Level | å…ç–«æ°´å¹³ | Individual immunity level (High/Medium/Low, based on medical tests). | ä¸ªäººå…ç–«æ°´å¹³ï¼ˆé«˜/ä¸­/ä½ï¼ŒåŸºäºåŒ»å­¦æ£€æµ‹ï¼‰ã€‚ |
    | Age | å¹´é¾„ | Patient's age (0-120 years). | æ‚£è€…å¹´é¾„ï¼ˆ0-120å²ï¼‰ã€‚ |

    #### 3. Data Cleaning Rules | æ•°æ®æ¸…æ´—è§„åˆ™
    - **Invalid Dates | æ— æ•ˆæ—¥æœŸ**: Rows with unparseable dates (e.g., "2023/13/01") are filtered out. | æ— æ³•è§£æçš„æ—¥æœŸï¼ˆå¦‚â€œ2023/13/01â€ï¼‰å·²è¿‡æ»¤ã€‚
    - **Outliers | å¼‚å¸¸å€¼**: Rows with age >120, negative cases, or zero hospital capacity are removed. | å¹´é¾„>120ã€è´Ÿç—…ä¾‹æ•°ã€é›¶åºŠä½çš„è¡Œå·²åˆ é™¤ã€‚
    - **Missing Values | ç¼ºå¤±å€¼**: 
      - Categorical fields (e.g., Location) filled with mode. | åˆ†ç±»å­—æ®µï¼ˆå¦‚åŒºåŸŸï¼‰ç”¨ä¼—æ•°å¡«å……ã€‚
      - Numeric fields (e.g., Age) filled with median (or default values if all missing). | æ•°å€¼å­—æ®µï¼ˆå¦‚å¹´é¾„ï¼‰ç”¨ä¸­ä½æ•°å¡«å……ï¼ˆå…¨ç¼ºå¤±æ—¶ç”¨é»˜è®¤å€¼ï¼‰ã€‚

    #### 4. Ethics & Limitations | ä¼¦ç†ä¸å±€é™æ€§
    - **Ethics | ä¼¦ç†**: All data is aggregated and de-identified; no personal information is included. | æ‰€æœ‰æ•°æ®å·²èšåˆå’Œå»æ ‡è¯†åŒ–ï¼Œä¸å«ä¸ªäººä¿¡æ¯ã€‚
    - **Limitations | å±€é™æ€§**: 
      - Data only covers 2023-2024 (no long-term trend analysis). | æ•°æ®ä»…è¦†ç›–2023-2024å¹´ï¼ˆæ— é•¿æœŸè¶‹åŠ¿åˆ†æï¼‰ã€‚
      - No county-level granularity (only regional-level data). | æ— å¿çº§ç²’åº¦ï¼ˆä»…åŒºåŸŸçº§æ•°æ®ï¼‰ã€‚
    """

def save_processed_data(processed_data: Dict[str, pd.DataFrame], output_dir: str = "data", file_name: str = "processed_analysis_tables.pkl") -> None:
    """ä¿å­˜å¤„ç†åçš„åˆ†æè¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºç¦»çº¿å¤ç”¨ï¼‰"""
    import pickle  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…æœªä½¿ç”¨æ—¶åŠ è½½
    
    # æ„å»ºè¾“å‡ºè·¯å¾„
    project_root = os.getcwd()
    output_file_path = os.path.join(project_root, output_dir, file_name)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
    if not os.path.exists(os.path.join(project_root, output_dir)):
        os.makedirs(os.path.join(project_root, output_dir), exist_ok=True)
        print(f"ğŸ“ Created output directory: {os.path.join(project_root, output_dir)} | åˆ›å»ºè¾“å‡ºç›®å½•ï¼š{os.path.join(project_root, output_dir)}")
    
    # ä¿å­˜æ•°æ®
    try:
        with open(output_file_path, "wb") as output_file:
            pickle.dump(processed_data, output_file, protocol=pickle.HIGHEST_PROTOCOL)
        print(f"âœ… Processed data saved to: {output_file_path} | å¤„ç†åæ•°æ®å·²ä¿å­˜è‡³ï¼š{output_file_path}")
    except Exception as e:
        raise RuntimeError(f"âŒ Failed to save processed data: {str(e)} | ä¿å­˜å¤„ç†åæ•°æ®å¤±è´¥ï¼š{str(e)}") from e