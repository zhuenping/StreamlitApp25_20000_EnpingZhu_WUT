"""
Complete Data Processing Pipeline | å®Œæ•´æ•°æ®å¤„ç†æµç¨‹
- Data Cleaning | æ•°æ®æ¸…æ´—
- Feature Engineering | ç‰¹å¾å·¥ç¨‹
- Analysis Table Generation | åˆ†æè¡¨ç”Ÿæˆ
- Filter Integration with Default Values | å«é»˜è®¤å€¼çš„è¿‡æ»¤å™¨é›†æˆï¼ˆè§£å†³å‚æ•°ç¼ºå¤±é—®é¢˜ï¼‰
"""
import pandas as pd
import numpy as np
from typing import Dict, Tuple
from utils.io import load_raw_dataset

def clean_raw_data(raw_df: pd.DataFrame) -> pd.DataFrame:
    """æ•°æ®æ¸…æ´—ï¼šå¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€æ ¼å¼æ ‡å‡†åŒ–"""
    clean_df = raw_df.copy()
    print("=== Starting Data Cleaning | å¼€å§‹æ•°æ®æ¸…æ´— ===")
    
    # 1. å­—æ®µåæ ‡å‡†åŒ–ï¼ˆå°å†™+ä¸‹åˆ’çº¿ï¼Œé¿å…å¤§å°å†™å†²çªï¼‰
    clean_df.columns = [col.lower().replace(" ", "_") for col in clean_df.columns]
    print("âœ… Field name standardized | å­—æ®µåæ ‡å‡†åŒ–å®Œæˆ")
    
    # 2. æ—¥æœŸå­—æ®µå¤„ç†ï¼ˆç»Ÿä¸€æ ¼å¼ï¼Œè¿‡æ»¤æ— æ•ˆæ—¥æœŸï¼‰
    clean_df["date_of_onset"] = pd.to_datetime(
        clean_df["date_of_onset"],
        errors="coerce",  # æ— æ•ˆæ—¥æœŸè½¬ä¸ºNaT
        format="mixed"    # è‡ªåŠ¨è¯†åˆ«æ—¥æœŸæ ¼å¼
    )
    invalid_date_count = clean_df["date_of_onset"].isnull().sum()
    if invalid_date_count > 0:
        clean_df = clean_df.dropna(subset=["date_of_onset"])
        print(f"âœ… Filtered {invalid_date_count} invalid dates | è¿‡æ»¤{invalid_date_count}è¡Œæ— æ•ˆæ—¥æœŸï¼Œå‰©ä½™{len(clean_df)}è¡Œ")
    
    # 3. åˆ†ç±»å­—æ®µå¤„ç†ï¼ˆå»é‡ã€å¡«å……ç¼ºå¤±å€¼ï¼‰
    categorical_fields = ["location", "ses", "immunity_level"]
    for field in categorical_fields:
        # ç»Ÿä¸€æ ¼å¼ï¼šé¦–å­—æ¯å¤§å†™ï¼Œå»é™¤å‰åç©ºæ ¼
        clean_df[field] = clean_df[field].str.title().str.strip()
        # å¡«å……ç¼ºå¤±å€¼ï¼šç”¨ä¼—æ•°ï¼ˆæ— ä¼—æ•°æ—¶ç”¨"Unknown"ï¼‰
        missing_count = clean_df[field].isnull().sum()
        if missing_count > 0:
            mode_value = clean_df[field].mode()[0] if not clean_df[field].mode().empty else "Unknown"
            clean_df[field] = clean_df[field].fillna(mode_value)
            print(f"âœ… Filled {missing_count} missing values for '{field}' with mode '{mode_value}' | å¡«å……'{field}'å­—æ®µ{missing_count}ä¸ªç¼ºå¤±å€¼ï¼ˆä¼—æ•°ï¼š{mode_value}ï¼‰")
    
    # 4. äºŒè¿›åˆ¶å­—æ®µå¤„ç†ï¼ˆç¡®ä¿0/1ç¼–ç ï¼Œå¡«å……ç¼ºå¤±å€¼ï¼‰
    binary_fields = ["chronic_conditions", "vaccination_status"]
    for field in binary_fields:
        # è½¬ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— æ•ˆå€¼è½¬ä¸ºNaN
        clean_df[field] = pd.to_numeric(clean_df[field], errors="coerce")
        # å¡«å……ç¼ºå¤±å€¼ä¸º0ï¼ˆé»˜è®¤"æœªæ‚£ç—…"/"æœªæ¥ç§"ï¼‰
        clean_df[field] = clean_df[field].fillna(0).astype(int)
        # è¿‡æ»¤é0/1å€¼
        invalid_val_count = len(clean_df[~clean_df[field].isin([0, 1])])
        if invalid_val_count > 0:
            clean_df = clean_df[clean_df[field].isin([0, 1])]
            print(f"âœ… Filtered {invalid_val_count} invalid values for '{field}' (only 0/1 allowed) | è¿‡æ»¤'{field}'å­—æ®µ{invalid_val_count}ä¸ªé0/1å€¼")
    
    # 5. æ•°å€¼å­—æ®µå¤„ç†ï¼ˆå¼‚å¸¸å€¼è¿‡æ»¤+ç¼ºå¤±å€¼å¡«å……ï¼‰
    numeric_fields = ["daily_new_cases", "hospital_capacity", "hospitalization_requirement", "age"]
    # ä¸šåŠ¡é»˜è®¤å€¼ï¼ˆå…¨ç¼ºå¤±æ—¶ä½¿ç”¨ï¼Œé¿å…ä¸­ä½æ•°ä¸ºNaNï¼‰
    default_values = {
        "daily_new_cases": 0,
        "hospital_capacity": 100,  # åŒ»é™¢é»˜è®¤100å¼ åºŠ
        "hospitalization_requirement": 10,
        "age": 35  # æˆå¹´äººé»˜è®¤å¹´é¾„
    }
    
    for field in numeric_fields:
        # è½¬ä¸ºæ•°å€¼ç±»å‹
        clean_df[field] = pd.to_numeric(clean_df[field], errors="coerce")
        # è®¡ç®—ä¸­ä½æ•°ï¼ˆå…¨ç¼ºå¤±æ—¶ç”¨é»˜è®¤å€¼ï¼‰
        median_value = clean_df[field].median()
        if pd.isna(median_value):
            median_value = default_values[field]
            print(f"âš ï¸ All values missing for '{field}', using default {median_value} | '{field}'å…¨ä¸ºç¼ºå¤±å€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼{median_value}")
        
        # å¡«å……ç¼ºå¤±å€¼
        missing_count = clean_df[field].isnull().sum()
        if missing_count > 0:
            clean_df[field] = clean_df[field].fillna(median_value)
            print(f"âœ… Filled {missing_count} missing values for '{field}' with {median_value:.1f} | å¡«å……'{field}'å­—æ®µ{missing_count}ä¸ªç¼ºå¤±å€¼ï¼ˆå€¼ï¼š{median_value:.1f}ï¼‰")
        
        # è¿‡æ»¤å¼‚å¸¸å€¼
        if field == "daily_new_cases":
            # ç—…ä¾‹æ•°ä¸èƒ½ä¸ºè´Ÿ
            invalid_count = len(clean_df[clean_df[field] < 0])
            clean_df = clean_df[clean_df[field] >= 0]
        elif field == "age":
            # å¹´é¾„èŒƒå›´ï¼š0-120å²
            invalid_count = len(clean_df[(clean_df[field] < 0) | (clean_df[field] > 120)])
            clean_df = clean_df[(clean_df[field] >= 0) & (clean_df[field] <= 120)]
        elif "capacity" in field or "requirement" in field:
            # å®¹é‡/éœ€æ±‚ä¸èƒ½â‰¤0
            invalid_count = len(clean_df[clean_df[field] <= 0])
            clean_df = clean_df[clean_df[field] > 0]
        
        if invalid_count > 0:
            print(f"âœ… Filtered {invalid_count} outliers for '{field}' | è¿‡æ»¤'{field}'å­—æ®µ{invalid_count}ä¸ªå¼‚å¸¸å€¼")
    
    # æœ€ç»ˆæ ¡éªŒï¼šæ¸…æ´—åæ•°æ®ä¸èƒ½ä¸ºç©º
    if len(clean_df) == 0:
        raise ValueError("âŒ No valid data left after cleaning! Check raw data quality | æ•°æ®æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®ï¼è¯·æ£€æŸ¥åŸå§‹æ•°æ®è´¨é‡")
    
    print(f"=== Data Cleaning Completed | æ•°æ®æ¸…æ´—å®Œæˆ ===")
    print(f"Cleaned Data: {len(clean_df)} rows Ã— {len(clean_df.columns)} columns | æ¸…æ´—åæ•°æ®ï¼š{len(clean_df)}è¡Œ Ã— {len(clean_df.columns)}åˆ—")
    return clean_df

# åœ¨create_business_featureså‡½æ•°ä¸­ä¿®æ”¹resource_loadè®¡ç®—
def create_business_features(clean_df: pd.DataFrame) -> pd.DataFrame:
    """ç‰¹å¾å·¥ç¨‹ï¼šç”Ÿæˆä¸šåŠ¡æ‰€éœ€ç‰¹å¾ï¼ˆæ”¯æ’‘å¯è§†åŒ–åˆ†æï¼‰"""
    feature_df = clean_df.copy()
    print("=== Starting Feature Engineering | å¼€å§‹ç‰¹å¾å·¥ç¨‹ ===")
    
    # 1. æ—¶é—´ç‰¹å¾ï¼ˆå¹´/æœˆ/å­£èŠ‚ï¼Œæ”¯æ’‘æ—¶é—´åºåˆ—åˆ†æï¼‰
    feature_df["year"] = feature_df["date_of_onset"].dt.year.astype(int)
    feature_df["month"] = feature_df["date_of_onset"].dt.month.astype(int)
    # å­£èŠ‚æ˜ å°„ï¼ˆåŒ—åŠçƒï¼š12-2æœˆå†¬å­£ï¼Œ3-5æœˆæ˜¥å­£ï¼Œ6-8æœˆå¤å­£ï¼Œ9-11æœˆç§‹å­£ï¼‰
    season_mapping = {1: "Winter", 2: "Winter", 3: "Spring", 4: "Spring", 5: "Spring",
                      6: "Summer", 7: "Summer", 8: "Summer", 9: "Autumn", 10: "Autumn",
                      11: "Autumn", 12: "Winter"}
    feature_df["season"] = feature_df["month"].map(season_mapping)
    print("âœ… Time features generated: year, month, season | ç”Ÿæˆæ—¶é—´ç‰¹å¾ï¼šå¹´ä»½ã€æœˆä»½ã€å­£èŠ‚")
    
    # 2. å¹´é¾„ç»„ç‰¹å¾ï¼ˆæ”¯æ’‘ç–«è‹—æ•ˆæœçš„å¹´é¾„å·®å¼‚åˆ†æï¼Œç¡®ä¿66+å²åˆ†ç»„ï¼‰
    feature_df["age_group"] = pd.cut(
        feature_df["age"],
        bins=[0, 18, 65, 120],  # æ˜ç¡®åˆ’åˆ†ï¼šå„¿ç«¥(0-18)ã€æˆäºº(19-65)ã€è€äºº(66+)
        labels=["Child (0-18)", "Adult (19-65)", "Elderly (66+)"],
        include_lowest=True  # åŒ…å«å·¦è¾¹ç•Œï¼ˆ0å²ï¼‰
    )
    print("âœ… Age group feature generated: Child (0-18), Adult (19-65), Elderly (66+) | ç”Ÿæˆå¹´é¾„ç»„ç‰¹å¾ï¼šå„¿ç«¥ã€æˆäººã€è€äºº")
    
    # 3. ç–«è‹—è¦†ç›–ç‡ç‰¹å¾ï¼ˆåŒºåŸŸ-å¹´åº¦ç»´åº¦ï¼Œæ”¯æ’‘ç–«è‹—æ•ˆæœåˆ†æï¼‰
    vaccine_coverage = feature_df.groupby(["year", "location"])["vaccination_status"].apply(
        lambda x: (x == 1).sum() / len(x) if len(x) > 0 else 0  # æ¥ç§ç‡=æ¥ç§äººæ•°/æ€»äººæ•°
    ).reset_index(name="vaccine_coverage")
    # åˆå¹¶å›ä¸»è¡¨
    feature_df = pd.merge(
        feature_df,
        vaccine_coverage,
        on=["year", "location"],
        how="left"
    )
    # ä¿ç•™3ä½å°æ•°ï¼Œå¡«å……ç¼ºå¤±å€¼ä¸º0
    feature_df["vaccine_coverage"] = np.round(feature_df["vaccine_coverage"].fillna(0), 3)
    print("âœ… Vaccine coverage feature generated | ç”Ÿæˆç–«è‹—è¦†ç›–ç‡ç‰¹å¾")
    
    # 4. åŒ»ç–—èµ„æºè´Ÿè·ç‰¹å¾ï¼ˆæ”¯æ’‘èµ„æºç´§å¼ åº¦åˆ†æï¼‰
    # ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨æ•°æ®ä¸­çš„å®é™…å€¼ï¼Œæ·»åŠ å®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿è®¡ç®—å‡†ç¡®æ€§
    if "resource_utilization" in feature_df.columns and not feature_df["resource_utilization"].isnull().all():
        # å¦‚æœæ•°æ®ä¸­å·²æœ‰èµ„æºåˆ©ç”¨ç‡å­—æ®µä¸”ä¸å…¨ä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨å¹¶æ ‡å‡†åŒ–åˆ°åˆç†èŒƒå›´
        feature_df["resource_load"] = np.clip(feature_df["resource_utilization"] / 100, 0, 5)  # ç¡®ä¿èŒƒå›´åœ¨0-5ä¹‹é—´
        print("âœ… Resource load feature generated from existing resource_utilization | ä»ç°æœ‰å­—æ®µç”Ÿæˆèµ„æºè´Ÿè·ç‰¹å¾")
    else:
        # å¦åˆ™ä½¿ç”¨ä½é™¢éœ€æ±‚/åŒ»é™¢å®¹é‡çš„è®¡ç®—
        # æ·»åŠ å®‰å…¨æ£€æŸ¥ï¼Œé¿å…é™¤ä»¥0
        feature_df["resource_load"] = np.where(
            feature_df["hospital_capacity"] > 0,  # å®‰å…¨æ£€æŸ¥
            feature_df["hospitalization_requirement"] / feature_df["hospital_capacity"],
            0  # é»˜è®¤å€¼
        )
    # ä¿ç•™3ä½å°æ•°ï¼Œé™åˆ¶æœ€å¤§å€¼ä¸º5ï¼ˆé¿å…æç«¯å€¼å½±å“ï¼‰
    feature_df["resource_load"] = np.round(feature_df["resource_load"], 3).clip(upper=5)
    print(f"âœ… Resource load feature generated | ç”Ÿæˆèµ„æºè´Ÿè·ç‰¹å¾ï¼šå¹³å‡å€¼={feature_df['resource_load'].mean():.3f}ï¼ŒèŒƒå›´={feature_df['resource_load'].min():.3f}-{feature_df['resource_load'].max():.3f}")
    
    # 5. ä¼ æ’­ç‡ç‰¹å¾ï¼ˆå­£èŠ‚ç»´åº¦ï¼Œæ”¯æ’‘ä¼ æ’­è¶‹åŠ¿åˆ†æï¼‰
    season_transmission_mapping = {"Winter": 1.2, "Spring": 0.9, "Summer": 0.6, "Autumn": 0.8}
    feature_df["transmission_rate"] = feature_df["season"].map(season_transmission_mapping)
    # ä¿ç•™2ä½å°æ•°
    feature_df["transmission_rate"] = np.round(feature_df["transmission_rate"], 2)
    print("âœ… Transmission rate feature generated | ç”Ÿæˆå­£èŠ‚ä¼ æ’­ç‡ç‰¹å¾")
    
    print("=== Feature Engineering Completed | ç‰¹å¾å·¥ç¨‹å®Œæˆ ===")
    print(f"Feature Data: {len(feature_df)} rows Ã— {len(feature_df.columns)} columns | ç‰¹å¾åæ•°æ®ï¼š{len(feature_df)}è¡Œ Ã— {len(feature_df.columns)}åˆ—")
    return feature_df

# åœ¨generate_analysis_tableså‡½æ•°ä¸­ä¿®æ”¹region_ses_tableç”Ÿæˆ
def generate_analysis_tables(feature_df: pd.DataFrame, year_filter: list, region_filter: list) -> Dict[str, pd.DataFrame]:
    """ç”Ÿæˆåˆ†æè¡¨ï¼ˆé›†æˆè¿‡æ»¤å™¨ï¼Œåº”ç”¨å¹´ä»½å’ŒåŒºåŸŸç­›é€‰ï¼‰"""
    analysis_tables = {}
    print("=== Starting Analysis Table Generation | å¼€å§‹ç”Ÿæˆåˆ†æè¡¨ ===")
    
    # åº”ç”¨è¿‡æ»¤å™¨ï¼ˆå¹´ä»½+åŒºåŸŸï¼‰
    filtered_df = feature_df.copy()
    if year_filter:
        filtered_df = filtered_df[filtered_df["year"].isin(year_filter)]
        print(f"ğŸ” Filtered by years: {year_filter} | æŒ‰å¹´ä»½è¿‡æ»¤ï¼š{year_filter}")
    if region_filter:
        filtered_df = filtered_df[filtered_df["location"].isin(region_filter)]
        print(f"ğŸ” Filtered by regions: {region_filter} | æŒ‰åŒºåŸŸè¿‡æ»¤ï¼š{region_filter}")
    
    # æ ¡éªŒè¿‡æ»¤åæ•°æ®æ˜¯å¦ä¸ºç©º
    if len(filtered_df) == 0:
        raise ValueError("âŒ No data left after filtering! Adjust filter criteria | è¿‡æ»¤åæ— æ•°æ®ï¼è¯·è°ƒæ•´è¿‡æ»¤æ¡ä»¶")
    
    # 1. æ—¶é—´åºåˆ—è¡¨ï¼ˆå¹´-æœˆ-åŒºåŸŸç»´åº¦ï¼Œæ”¯æ’‘æ¦‚è§ˆé¡µè¶‹åŠ¿å›¾ï¼‰
    # ä¿®æ”¹generate_analysis_tableså‡½æ•°ä¸­çš„timeseries_tableç”Ÿæˆéƒ¨åˆ†
    timeseries_table = filtered_df.groupby(["year", "month", "location"]).agg({
    "daily_new_cases": "sum",          # æœˆæ–°å¢ç—…ä¾‹æ€»æ•°
    "transmission_rate": "mean",       # æœˆå¹³å‡ä¼ æ’­ç‡
    "vaccine_coverage": "mean",         # æœˆå¹³å‡ç–«è‹—è¦†ç›–ç‡
    "resource_load": "mean"            # æœˆå¹³å‡èµ„æºè´Ÿè·
    }).reset_index().sort_values(["year", "month", "location"])
    
    # æ·»åŠ ç¼ºå¤±çš„dateåˆ—
    timeseries_table["date"] = pd.to_datetime(
    timeseries_table["year"].astype(str) + '-' + timeseries_table["month"].astype(str) + '-01'
    )
    analysis_tables["timeseries"] = timeseries_table
    print(f"âœ… Time series table generated: {len(timeseries_table)} rows | ç”Ÿæˆæ—¶é—´åºåˆ—è¡¨ï¼š{len(timeseries_table)}è¡Œ")
    
    # 2. åŒºåŸŸ-SESè¡¨ï¼ˆåŒºåŸŸ-SES-å­£èŠ‚ç»´åº¦ï¼Œæ”¯æ’‘æ·±åº¦åˆ†æé¡µæŸ±çŠ¶å›¾ï¼‰
    region_ses_table = filtered_df.groupby(["location", "ses", "season"]).agg({
        "daily_new_cases": "sum",          # å­£èŠ‚ç—…ä¾‹æ€»æ•°
        "vaccine_coverage": "mean",         # å­£èŠ‚å¹³å‡ç–«è‹—è¦†ç›–ç‡
        "resource_load": "mean",            # å­£èŠ‚å¹³å‡èµ„æºè´Ÿè·
        "age": "median"                    # å­£èŠ‚å¹´é¾„ä¸­ä½æ•°
    }).reset_index()
    
    # æ·»åŠ æ ·æœ¬é‡åˆ—ï¼Œç”¨äºå‚è€ƒ
    sample_size = filtered_df.groupby(["location", "ses", "season"]).size().reset_index(name="sample_size")
    region_ses_table = pd.merge(region_ses_table, sample_size, on=["location", "ses", "season"])
    
    analysis_tables["region_ses"] = region_ses_table
    print(f"âœ… Region-SES table generated: {len(region_ses_table)} rows | ç”ŸæˆåŒºåŸŸ-SESè¡¨ï¼š{len(region_ses_table)}è¡Œ")
    
    # 3. ç–«è‹—æ•ˆæœè¡¨ï¼ˆæ ¸å¿ƒï¼šæ¥ç§çŠ¶æ€-å…ç–«æ°´å¹³-å¹´é¾„ç»„ç»´åº¦ï¼Œç¡®ä¿66+å²æ˜¾ç¤ºï¼‰
    vaccine_effect_df = filtered_df.dropna(subset=["vaccination_status", "immunity_level", "age_group"])
    vaccine_effect_table = vaccine_effect_df.groupby(
        ["vaccination_status", "immunity_level", "age_group"],
        observed=True  # è§£å†³åˆ†ç±»å­—æ®µèšåˆè­¦å‘Š
    ).agg({
        "daily_new_cases": "mean",         # å¹³å‡æ¯æ—¥æ–°å¢ç—…ä¾‹ï¼ˆYè½´ï¼‰
        "transmission_rate": "mean"        # å¹³å‡ä¼ æ’­ç‡ï¼ˆç‚¹å¤§å°ï¼‰
    }).reset_index()
    # æ¥ç§çŠ¶æ€æ˜ å°„ä¸ºå¯è¯»æ€§æ ‡ç­¾ï¼ˆ0â†’æœªæ¥ç§ï¼Œ1â†’å·²æ¥ç§ï¼‰
    vaccine_effect_table["vaccination_status"] = vaccine_effect_table["vaccination_status"].map({
        0: "Unvaccinated",
        1: "Vaccinated"
    })
    analysis_tables["vaccine_effect"] = vaccine_effect_table
    print(f"âœ… Vaccine effect table generated: {len(vaccine_effect_table)} rows | ç”Ÿæˆç–«è‹—æ•ˆæœè¡¨ï¼š{len(vaccine_effect_table)}è¡Œ")
    
    # 4. KPIè¡¨ï¼ˆå…¨å±€æ ¸å¿ƒæŒ‡æ ‡ï¼Œæ”¯æ’‘æ¦‚è§ˆé¡µæŒ‡æ ‡å¡ï¼‰ - ç§»é™¤é«˜å±äººç¾¤ç»Ÿè®¡
    total_cases = filtered_df["daily_new_cases"].sum()
    avg_vaccine_coverage = np.round(filtered_df["vaccine_coverage"].fillna(0).mean(), 3)
    avg_resource_load = np.round(filtered_df["resource_load"].fillna(0).mean(), 3)
    # é«˜å³°å­£èŠ‚ç—…ä¾‹æ•°ï¼ˆå•å­£èŠ‚æœ€å¤§ç—…ä¾‹æ•°ï¼‰
    peak_season_cases = filtered_df.groupby("season")["daily_new_cases"].sum().max() if not filtered_df.empty else 0
    
    kpi_table = pd.DataFrame({
        "metric": [
            "Total Cases | æ€»ç—…ä¾‹æ•°",
            "Average Vaccine Coverage | å¹³å‡ç–«è‹—è¦†ç›–ç‡",
            "Average Resource Load | å¹³å‡èµ„æºè´Ÿè·",
            "Peak Season Cases | é«˜å³°å­£èŠ‚ç—…ä¾‹æ•°"
        ],
        "value": [total_cases, avg_vaccine_coverage, avg_resource_load, peak_season_cases],
        "unit": [
            "Cases | ä¾‹",
            "Ratio | æ¯”ä¾‹",
            "Ratio | æ¯”ä¾‹",
            "Cases | ä¾‹"
        ],
        "description": [
            "Total new cases in filtered data | è¿‡æ»¤åæ•°æ®ä¸­çš„æ€»æ–°å¢ç—…ä¾‹æ•°",
            "Average vaccination rate across regions/years | æ‰€æœ‰åŒºåŸŸ/å¹´ä»½çš„å¹³å‡ç–«è‹—æ¥ç§ç‡",
            "Average ratio of hospitalization requirement to capacity | ä½é™¢éœ€æ±‚ä¸åŒ»é™¢å®¹é‡çš„å¹³å‡æ¯”ä¾‹",
            "Maximum number of cases in any season | ä»»æ„å­£èŠ‚çš„æœ€å¤§ç—…ä¾‹æ•°"
        ]
    })
    analysis_tables["kpi"] = kpi_table
    print("âœ… KPI table generated | ç”ŸæˆKPIè¡¨")
    
    # 5. åŸå§‹ç‰¹å¾è¡¨ï¼ˆæ”¯æ’‘æ•°æ®è´¨é‡æ£€æŸ¥ï¼‰
    analysis_tables["raw_feature"] = filtered_df
    print(f"âœ… Raw feature table retained: {len(filtered_df)} rows | ä¿ç•™åŸå§‹ç‰¹å¾è¡¨ï¼š{len(filtered_df)}è¡Œ")
    
    print("=== Analysis Table Generation Completed | åˆ†æè¡¨ç”Ÿæˆå®Œæˆ ===")
    print(f"Total tables generated: {len(analysis_tables)} | å…±ç”Ÿæˆ{len(analysis_tables)}ä¸ªåˆ†æè¡¨")
    return analysis_tables

# åœ¨æ•°æ®å¤„ç†æµç¨‹ä¸­ï¼Œç¡®ä¿ä¸ºæ—¶é—´åºåˆ—æ•°æ®åˆ›å»ºå¿…è¦çš„æ—¶é—´åˆ—
def get_processed_data(year_filter=None, region_filter=None):
    try:
        # æ­¥éª¤1ï¼šåŠ è½½åŸå§‹æ•°æ®
        raw_data = load_raw_dataset()
        # æ­¥éª¤2ï¼šæ•°æ®æ¸…æ´—
        cleaned_data = clean_raw_data(raw_data)
        # æ­¥éª¤3ï¼šç‰¹å¾å·¥ç¨‹
        feature_data = create_business_features(cleaned_data)
        
        # æ­¥éª¤4ï¼šå¤„ç†é»˜è®¤è¿‡æ»¤å‚æ•°ï¼ˆæœªä¼ å…¥åˆ™ä½¿ç”¨å…¨é‡æ•°æ®ï¼‰
        if year_filter is None:
            year_filter = feature_data["year"].unique().tolist()
            print(f"â„¹ï¸ No year filter provided, using all years: {year_filter} | æœªæä¾›å¹´ä»½è¿‡æ»¤ï¼Œä½¿ç”¨æ‰€æœ‰å¹´ä»½ï¼š{year_filter}")
        if region_filter is None:
            region_filter = feature_data["location"].unique().tolist()
            print(f"â„¹ï¸ No region filter provided, using all regions: {region_filter} | æœªæä¾›åŒºåŸŸè¿‡æ»¤ï¼Œä½¿ç”¨æ‰€æœ‰åŒºåŸŸï¼š{region_filter}")
        
        # æ­¥éª¤5ï¼šç”Ÿæˆåˆ†æè¡¨ï¼ˆåº”ç”¨è¿‡æ»¤ï¼‰
        analysis_tables = generate_analysis_tables(feature_data, year_filter, region_filter)
        
        return feature_data, analysis_tables
    except Exception as e:
        # æ•è·å¼‚å¸¸å¹¶é‡æ–°æŠ›å‡ºï¼ˆé™„åŠ ä¸Šä¸‹æ–‡ï¼‰
        raise RuntimeError(f"End-to-end data processing failed: {str(e)} | ç«¯åˆ°ç«¯æ•°æ®å¤„ç†å¤±è´¥ï¼š{str(e)}") from e